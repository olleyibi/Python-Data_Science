{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "### 3. The *width and depth* of the algorithm. Add as many additional layers as you need to reach 5 hidden layers. Moreover, adjust the width of the algorithm as you find suitable. How does the validation accuracy change? What about the time it took the algorithm to train?\n",
    "\n",
    "**Solution**\n",
    "\n",
    "This exercise is pretty much the same as the previous one. However, it will get us to a much deeper net. As we noted in the previous exercise, you a deeeper net may need to be wider to produce better results.\n",
    "\n",
    "We tried with 1000 hidden units in each layer and 5 hidden layers.\n",
    "\n",
    "The result (as you can see below) is that our model's training was going very well, until it overfit. It did so by quite a lot.\n",
    "\n",
    "It took my personal computer around 5-6 minutes to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network for MNIST Classification\n",
    "\n",
    "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" for machine learning because for most students it is their first example. The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional networks. The dataset provides 28x28 images of handwritten digits (1 per image) and the goal is to write an algorithm that detects which digit is written. Since there are only 10 digits, this is a classification problem with 10 classes. In order to exemplify what we've talked about in this section, we will build a network with 2 hidden layers between inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\iolley2\\AppData\\Local\\Temp\\ipykernel_7120\\120138914.py:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline the model\n",
    "\n",
    "The whole code is in one cell, so you can simply rerun this cell (instead of the whole notebook) and train a new model.\n",
    "The tf.reset_default_graph() function takes care of clearing the old parameters. From there on, a completely new training starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\iolley2\\AppData\\Local\\Temp\\ipykernel_7120\\3540331101.py:7: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\iolley2\\AppData\\Local\\Temp\\ipykernel_7120\\3540331101.py:64: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch 1. Mean loss: 0.232. Validation loss: 0.120. Validation accuracy: 96.62%\n",
      "Epoch 2. Mean loss: 0.109. Validation loss: 0.115. Validation accuracy: 97.22%\n",
      "Epoch 3. Mean loss: 0.078. Validation loss: 0.094. Validation accuracy: 97.48%\n",
      "Epoch 4. Mean loss: 0.061. Validation loss: 0.091. Validation accuracy: 97.64%\n",
      "Epoch 5. Mean loss: 0.050. Validation loss: 0.078. Validation accuracy: 97.76%\n",
      "Epoch 6. Mean loss: 0.046. Validation loss: 0.093. Validation accuracy: 97.80%\n",
      "End of training.\n",
      "Training time: 122.2458119392395 seconds\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "hidden_layer_size = 1000\n",
    "\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "\n",
    "inputs = tf.compat.v1.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.compat.v1.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "\n",
    "weights_1 = tf.compat.v1.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.compat.v1.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "\n",
    "\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "\n",
    "weights_2 = tf.compat.v1.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.compat.v1.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "\n",
    "\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "# ***************************************************************************************************\n",
    "# SOLUTION\n",
    "\n",
    "\n",
    "weights_3 = tf.compat.v1.get_variable(\"weights_3\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_3 = tf.compat.v1.get_variable(\"biases_3\", [hidden_layer_size])\n",
    "\n",
    "\n",
    "outputs_3 = tf.nn.relu(tf.matmul(outputs_2,weights_3) + biases_3)\n",
    "\n",
    "\n",
    "\n",
    "weights_4 = tf.compat.v1.get_variable(\"weights_4\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_4 = tf.compat.v1.get_variable(\"biases_4\", [hidden_layer_size])\n",
    "\n",
    "\n",
    "outputs_4 = tf.nn.relu(tf.matmul(outputs_3,weights_4) + biases_4)\n",
    "\n",
    "\n",
    "\n",
    "weights_5 = tf.compat.v1.get_variable(\"weights_5\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_5 = tf.compat.v1.get_variable(\"biases_5\", [hidden_layer_size])\n",
    "\n",
    "\n",
    "outputs_5 = tf.nn.relu(tf.matmul(outputs_4,weights_5) + biases_5)\n",
    "\n",
    "\n",
    "\n",
    "weights_6 = tf.compat.v1.get_variable(\"weights_6\",[hidden_layer_size,output_size])\n",
    "biases_6 = tf.compat.v1.get_variable(\"biases_6\", [output_size])\n",
    "\n",
    "\n",
    "outputs = tf.matmul(outputs_5, weights_6) + biases_6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "\n",
    "\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "optimize = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)\n",
    "\n",
    "\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "\n",
    "initializer = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "batches_number = mnist.train._num_examples // batch_size\n",
    "\n",
    "\n",
    "max_epochs = 15\n",
    "\n",
    "\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for epoch_counter in range(max_epochs):\n",
    "    \n",
    "    \n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    \n",
    "    for batch_counter in range(batches_number):\n",
    "        \n",
    "        \n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        \n",
    "        _, batch_loss = sess.run([optimize, mean_loss], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        \n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "    \n",
    "    curr_epoch_loss /= batches_number\n",
    "    \n",
    "    input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    \n",
    "    \n",
    "    validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "        feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    \n",
    "    \n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Mean loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    \n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "        \n",
    "    prev_validation_loss = validation_loss\n",
    "\n",
    "    \n",
    "print('End of training.')\n",
    "\n",
    "\n",
    "print(\"Training time: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "As we discussed in the lectures, after training on the training and validation sets, we test the final prediction power of our model by running it on the test dataset that the algorithm has not seen before.\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset. The test is the absolute final instance. You should not test before you are completely done with adjusting your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.67%\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "test_accuracy = sess.run([accuracy], \n",
    "    feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "\n",
    "# print (test_accuracy)\n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "\n",
    "\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the initial model and hyperparameters given in this notebook, the final test accuracy should be roughly between 97% and 98%. Each time the code is rerunned, we get a different accuracy as the batches are shuffled, the weights are initialized in a different way, etc.\n",
    "\n",
    "Finally, we have intentionally reached a suboptimal solution, so you can have space to build on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF1]",
   "language": "python",
   "name": "conda-env-py3-TF1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
