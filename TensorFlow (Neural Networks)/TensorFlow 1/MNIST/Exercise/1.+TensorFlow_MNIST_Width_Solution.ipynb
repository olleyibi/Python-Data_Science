{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "### 1. The *width* (the hidden layer size) of the algorithm. Try a hidden layer size of 200. How does the validation accuracy of the model change? What about the time it took the algorithm to train? Can you find a hidden layer size that does better?\n",
    "\n",
    "**Solution**\n",
    "\n",
    "Find the variable: \"hidden_layer_size\" and change it to 200.\n",
    "\n",
    "The validation accuracy is significantly higher (as the algorithm with 50 hidden units was too simple of a model).\n",
    "\n",
    "Naturally, it takes the algorithm much longer to train (unless early stopping is triggered too soon).\n",
    "\n",
    "A hidden layer size of 500 (and not only) works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network for MNIST Classification\n",
    "\n",
    "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" for machine learning because for most students it is their first example. The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional networks. The dataset provides 28x28 images of handwritten digits (1 per image) and the goal is to write an algorithm that detects which digit is written. Since there are only 10 digits, this is a classification problem with 10 classes. In order to exemplify what we've talked about in this section, we will build a network with 2 hidden layers between inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline the model\n",
    "\n",
    "The whole code is in one cell, so you can simply rerun this cell (instead of the whole notebook) and train a new model.\n",
    "The tf.reset_default_graph() function takes care of clearing the old parameters. From there on, a completely new training starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\iolley2\\Anaconda3\\envs\\py3-TF1\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\iolley2\\AppData\\Local\\Temp\\ipykernel_30840\\1544691392.py:35: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch 1. Mean loss: 0.276. Validation loss: 0.127. Validation accuracy: 96.44%\n",
      "Epoch 2. Mean loss: 0.103. Validation loss: 0.102. Validation accuracy: 97.00%\n",
      "Epoch 3. Mean loss: 0.070. Validation loss: 0.072. Validation accuracy: 97.90%\n",
      "Epoch 4. Mean loss: 0.050. Validation loss: 0.077. Validation accuracy: 97.58%\n",
      "End of training.\n",
      "Training time: 4.320937156677246 seconds\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "hidden_layer_size = 200\n",
    "\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "\n",
    "inputs = tf.compat.v1.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.compat.v1.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "\n",
    "weights_1 = tf.compat.v1.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.compat.v1.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "\n",
    "\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "\n",
    "weights_2 = tf.compat.v1.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.compat.v1.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "\n",
    "\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "\n",
    "weights_3 = tf.compat.v1.get_variable(\"weights_3\", [hidden_layer_size, output_size])\n",
    "biases_3 = tf.compat.v1.get_variable(\"biases_3\", [output_size])\n",
    "\n",
    "\n",
    "outputs = tf.matmul(outputs_2, weights_3) + biases_3\n",
    "\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "\n",
    "\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "optimize = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)\n",
    "\n",
    "\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "\n",
    "initializer = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "batches_number = mnist.train._num_examples // batch_size\n",
    "\n",
    "\n",
    "max_epochs = 15\n",
    "\n",
    "\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for epoch_counter in range(max_epochs):\n",
    "    \n",
    "    \n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    \n",
    "    for batch_counter in range(batches_number):\n",
    "        \n",
    "        \n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        \n",
    "        _, batch_loss = sess.run([optimize, mean_loss], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        \n",
    "        curr_epoch_loss += batch_loss\n",
    "        \n",
    "    curr_epoch_loss /= batches_number\n",
    "    \n",
    "    \n",
    "    input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    \n",
    "    \n",
    "    validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "        feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    \n",
    "    \n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Mean loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    \n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "        \n",
    "    prev_validation_loss = validation_loss\n",
    "\n",
    "    \n",
    "print('End of training.')\n",
    "\n",
    "\n",
    "print(\"Training time: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "As we discussed in the lectures, after training on the training and validation sets, we test the final prediction power of our model by running it on the test dataset that the algorithm has not seen before.\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset. The test is the absolute final instance. You should not test before you are completely done with adjusting your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.38%\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "test_accuracy = sess.run([accuracy], \n",
    "    feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "\n",
    "# print (test_accuracy)\n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "\n",
    "\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the initial model and hyperparameters given in this notebook, the final test accuracy should be roughly between 97% and 98%. Each time the code is rerunned, we get a different accuracy as the batches are shuffled, the weights are initialized in a different way, etc.\n",
    "\n",
    "Finally, we have intentionally reached a suboptimal solution, so you can have space to build on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF1]",
   "language": "python",
   "name": "conda-env-py3-TF1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
