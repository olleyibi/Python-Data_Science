# Function and methods
	Functions can take in multiple arguments/objects
		eg.
			function_name(arguments/object)
			
	Methods are applied at the end of a single object
		eg.
			object_name.method()


# Index and slicing
	Python position/index counts starts from '0' (zero)


# Data type identification
	Tuple = ()
	List = []
	Dictionary = {}


# Class, Object, Attributes
	Class: eg 								bike-maker, 		list class
	Object/instance: eg 							bikes, 			list
	attribute/properties: eg 						color, size, 		type(int,str,floats)
	Method is like a cosequential sequence applied to an object
		eg
										.turn_left()		.extend()
										.turn_right()		.index()


# SST / SSR / SSE
	# SST (sum of squares total) OR TSS (Total Sum of Squares)
		sumation of square diff between observed variavle and its mean
		Dispersion of observed variable around mean or measure of total variability of the data
		E(y(i) - ybar)^2

	# SSR (sum of squares Regressions) OR ESS (Explained Sum of Squares)
		sumation of square diff between predicted variavle and its mean
		Measure that describes how well a line fits the data
		E(yhat(i) - ybar)^2

	# SSE (sum of squares Error) OR RSS (Residual Sum of Squares)
		sumation of square diff between predicted variavle and its mean
		Measure the unexplained variability by the regression
		The smaller the error, the better the estimation power of regression
		E(y(i) - yhat(i))^2

	If SST = SSR....means regression captures all variability and is perfect
	
	SST = SSR + SSE
	
# R-Squared
	R^2 = SSR/SST
	This is a value/measure that quantifies the level of variability explained by the model
	It measures the goodness of fit of the model
	
# Adjusted R-Squared
	Always smaller than R-Squared
	This is a measure that penalizes a model for increasing the number of predictors used
	It is a basis for comparing models
		Adjusted R^2 = 1 - (1-R^2)*(n-1) / (n-p-1)
			n = number of observations
			p = number of predictors
	
# Linear Regression
	# Simple Linear Regression equations
		y = b0 + b1x :for every unit increase in x, y increases by b1
		y = b0 + b1[log(x)] :for every unit percent increase in x, y increases by b1
		log(y) = b0 + b1[log(x)] :for every unit percent percent increase in x, y increases by b1 percent
		log(y) = b0 + b1[log(x)] :for every unit increase in x, y increases by b1 percent
		
	# Multiple Linear Regression
		y = b0 + b1x1 + b2x2 +....... bnxn
		
	y: dependent variable/predicted
	x,x1,.....xn: independent variables/predictors
	b0: intercept
	b1,....bn: coefficient/slope
	
	A positive weight(b1,...bn) indicates that as a feature(x) increases in value, so does y
	A negative weight(b1,...bn) indicates that as a feature(x) decreases in value, so does y

# Hypothesis
	Regression coefficients
		Null Hypothesis:- H0 = b = 0 :: b is any coefficient/slope of a model (not intercept(b0)
			 A p-value < 0.05 signifies to reject the null hypothesis, i.e b is significantly/statistically different from 0(zero)
			 
	Z - Statistic:- Follows a normal distribution
	T - Statistic:- Follows a student T distribution
	F - Statistic:- Follows F distribution
		The lower the F - Statistic, the less significant the model is
		This is used to test for overall significance of the model
			F-test:
				Null Hypothesis H0:b1=b2......bn=0
				Alternate Hypothesis H1: atleast one bi != 0
					It implies that if all bi = 0, then no independent variable matter
					A p-value < 0.05 signifies to reject the null hypothesis, i.e atliest one bi is significantly/statistically different from 0(zero)

# Multicollinearity
	This is the occurrence of high intercorrelations among two or more independent variables in a multiple regression model
	This can be checked using VIF (variance inflation factor
		VIF = 1 :: no multicollinearity
		1 < VIF < 5 :: perfectly ok
		5,6,10 < VIF :: unacceptable (it varies from 5-10)
	
# Endogeneity
	Occurs when explanatory variables are omitted in the regression, which would result in the error term being correlated with the explanatory variables
					
# Homoscedastic
	This is a condition in which the variance of the residual, or error term, in a regression model is constant (no pattern when residuals are plotted)
	The most common fix for this is the log transformation

# Autocorrelation
	This refers to the degree of correlation between the values of the same variables across different observations in the data
	# Detecting it
		Manually:: This can be detected by plotting the residuals and looking for patterns in the plot, if none then no autocorrelation
		Using Durbin watson Test:: Usually between 0 and 4
						2 indicates no autocorelation
						<1 and >3  indicates the presence of autocorelation

# Standardization
	Also known as feature scaling or normalization
	Process of transforming data to a standard scale
	It involves subtracting the mean and dividing bt the standard deviation
	
# Overfitting and Underfitting
	Overfitting means a model has focused on the training set so muct that it 'missed the point'
		Random noise is captured
		High train accuracy and low test accuracy
		Solution: Split data into 2 (training and test set)
	Underfitting means the model has not captured the underlying logic of the data
		it does not have  a strong predictive power with low accuracy
		Low train accuracy and low test accuracy

# Non-Linear Model
	Quadratic
	Exponential
	Logistic

# Logistic Regression
	This implies that the possible outcomes are not numerical but rather categorical
	This predicts the probability of an event occuring
	
	p(X) = [e(b0 + b1x1 +.....+bnxn)]/[1 + e(b0 + b1x1 +.....+bnxn)] .................Logistic Regression Model
		When transformed:: [p(X)]/[1-p(X)] = e(b0 + b1x1 +.....+bnxn) ................ Logit Regression Model
			[p(X)]/[1-p(X)] is called ODDS
			Therfore:: ODDS = e(b0 + b1x1 +.....+bnxn)
				Implies:: log(ODDS) = b0 + b1x1 +.....+bnxn
					Log of ODDS is equal to a linear model
		
	Where:: p(X) represents the 'probability of and event 'X' occuring
	
# Likelihiid Function
	Estimates how likely a model describes the real underlying relationship of the variables
	The bigger the likelihood function, the higher the probability the model is correct
	* MLE (Maximum likelihood estimation)
		This tries to maximize the (log) likelihood function
	* LL_Null (Log likelihood-null)
		This is the log likelihood of a model with no independent variable
		y = b0
	* LLR (Log Likelihood ratio)
		Like F-statistic
		It measures if the model is statistically different from the LL_Null (ie useless model)
	* Pseudo R-Squared (Recall: AIC, BIC, McFadden's R-Squared[alternative name])
		A goog value is between 0.2 and 0.4
		




# NB
	It is not always appropriate to standardize dummy variables because scaling has no effect on their predictive power; but when scaled they lose all the dummy meaning
	
	# Model Assumptions
		Linear						Logistic
		Linearity					Non-Linear
		No Endogeneity					No Endogeneity
		Normality & Homoscedastic			Normality & Homoscedastic
		No Autocorrelation				No Autocorrelation
		No Multicollinearity				No Multicollinearity
