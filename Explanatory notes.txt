# Function and methods
	Functions can take in multiple arguments/objects
		eg.
			function_name(arguments/object)
			
	Methods are applied at the end of a single object
		eg.
			object_name.method()


# Index and slicing
	Python position/index counts starts from '0' (zero)


# Data type identification
	Tuple = ()
	List = []
	Dictionary = {}


# Class, Object, Attributes
	Class: eg 															bike-maker, 		list class
	Object/instance: eg 												bikes, 				list
	attribute/properties: eg 											color, size, 		type(int,str,floats)
	Method is like a cosequential sequence applied to an object
		eg
																		.turn_left()		.extend()
																		.turn_right()		.index()


# Types of Analysis
	* Exploratory
		Getting acquainted with the data without explicitely trying to explain anything
		Searching for patterns
		Planning method to use
		* Techniques used include
			Data Visualization
			Descriptive Statistics
			Clustering
	* Confirmatory and Explanatory
		Both explain a phenomeno
		Confirm hypothesis
		Validate previous research
		* Techniques Used includes
			Hypothesis testing
			Regression Analysis


# SST / SSR / SSE
	# SST (sum of squares total) OR TSS (Total Sum of Squares)
		sumation of square diff between observed variavle and its mean
		Dispersion of observed variable around mean or measure of total variability of the data
		E(y(i) - ybar)^2

	# SSR (sum of squares Regressions) OR ESS (Explained Sum of Squares)
		sumation of square diff between predicted variavle and its mean
		Measure that describes how well a line fits the data
		E(yhat(i) - ybar)^2

	# SSE (sum of squares Error) OR RSS (Residual Sum of Squares)
		sumation of square diff between predicted variavle and its mean
		Measure the unexplained variability by the regression
		The smaller the error, the better the estimation power of regression
		E(y(i) - yhat(i))^2

	If SST = SSR....means regression captures all variability and is perfect
	SST = SSR + SSE


# Preprocessing
	This is any manipulation of the dataset before running it through the model
	* Importance
		Helps improve compatibility with the used library
		Adjust imputs of diff magnitudes
		Helps with generalization
	* Types (transformations)
		Relatives
			Relative metrics are useful with time-series data. (e.g. stock prices, exchange rates)
		Logarithms
			Faster computation
			Lower order of magnitudes
			Clearer relationships
			Homogeneous variance
		Machine learning Preprocessing
			- On Numerical data
				* Standardization (Hepls with the difference in magnitude when working with numerical data)
					Also known as Feature scaling or Normalization
					This is the process of transforming data into a standard scale (subtract mean and divide by its standard deviation: results to a mean of 0 and standard deviation of 1)
				* Normalization
					Could invlove converting samples to a unit lehgth vector using either L1 or L2-norm
				* PCA (Principal Component Analysis)
					A dimension reduction technique used to combine several variables (that refers to the smabe bigger concept) into a bigger (latent) variable
					Also often have standardized output
				* Whitening
					Mostly performed after PCA
					Removes most underlying correlation between data points (useful when conceptually data should be uncorrelated)
			- On Categorical data
				* Ordinal Encoding
					Assign progressive values to each category/group
				* One-Hot Encoding (helps reduce correlation)
					It requires a lot of columns
					Prefered when data has few categories
					Consists of creating as many columns as posible values/groups
						Groups		cake	yoghurt		bread
						cake		1		0			0
						yoghurt		0		1			0
						bread		0		0			1
				* Binary Encoding
					Might have some unspecified correlation
					Preferable when data has many categories
					Convert the ordinal values from ordinal encoding to binary to form singe column for each digit
						Groups		ordinal		binary ->	var1	var2
						cake		1			01 ->		0		1
						yoghurt		2			10 ->		1		0
						bread		3			11 ->		1		1

	
# R-Squared
	R^2 = SSR/SST
	This is a value/measure that quantifies the level of variability explained by the model
	It measures the goodness of fit of the model
	

# Adjusted R-Squared
	Always smaller than R-Squared
	This is a measure that penalizes a model for increasing the number of predictors used
	It is a basis for comparing models
		Adjusted R^2 = 1 - (1-R^2)*(n-1) / (n-p-1)
			n = number of observations
			p = number of predictors

	
# Linear Regression
	# Simple Linear Regression equations
		y = b0 + b1x :for every unit increase in x, y increases by b1
		y = b0 + b1[log(x)] :for every unit percent increase in x, y increases by b1
		log(y) = b0 + b1[log(x)] :for every unit percent percent increase in x, y increases by b1 percent
		log(y) = b0 + b1[log(x)] :for every unit increase in x, y increases by b1 percent
		
	# Multiple Linear Regression
		y = b0 + b1x1 + b2x2 +....... bnxn
		
	y: dependent variable/predicted
	x,x1,.....xn: independent variables/predictors
	b0: intercept
	b1,....bn: coefficient/slope
	
	A positive weight(b1,...bn) indicates that as a feature(x) increases in value, so does y
	A negative weight(b1,...bn) indicates that as a feature(x) decreases in value, so does y


# Hypothesis
	Regression coefficients
		Null Hypothesis:- H0 = b = 0 :: b is any coefficient/slope of a model (not intercept(b0)
			 A p-value < 0.05 signifies to reject the null hypothesis, i.e b is significantly/statistically different from 0(zero)
			 
	Z - Statistic:- Follows a normal distribution
	T - Statistic:- Follows a student T distribution
	F - Statistic:- Follows F distribution
		The lower the F - Statistic, the less significant the model is
		This is used to test for overall significance of the model
			F-test:
				Null Hypothesis H0:b1=b2......bn=0
				Alternate Hypothesis H1: atleast one bi != 0
					It implies that if all bi = 0, then no independent variable matter
					A p-value < 0.05 signifies to reject the null hypothesis, i.e atliest one bi is significantly/statistically different from 0(zero)


# Multicollinearity
	This is the occurrence of high intercorrelations among two or more independent variables in a multiple regression model
	This can be checked using VIF (variance inflation factor
		VIF = 1 :: no multicollinearity
		1 < VIF < 5 :: perfectly ok
		5,6,10 < VIF :: unacceptable (it varies from 5-10)
	

# Endogeneity
	Occurs when explanatory variables are omitted in the regression, which would result in the error term being correlated with the explanatory variables
					

# Homoscedastic
	This is a condition in which the variance of the residual, or error term, in a regression model is constant (no pattern when residuals are plotted)
	The most common fix for this is the log transformation


# Autocorrelation
	This refers to the degree of correlation between the values of the same variables across different observations in the data
	# Detecting it
		Manually:: This can be detected by plotting the residuals and looking for patterns in the plot, if none then no autocorrelation
		Using Durbin watson Test:: 		Usually between 0 and 4
										2 indicates no autocorelation
										<1 and >3  indicates the presence of autocorelation


# Standardization
	Also known as feature scaling or normalization
	Process of transforming data to a standard scale
	It involves subtracting the mean and dividing bt the standard deviation
	

# Overfitting and Underfitting
	Overfitting means a model has focused on the training set so muct that it 'missed the point'
		Random noise is captured
		High train accuracy and low test accuracy
		Solution: Split data into 2 (training and test set)
	Underfitting means the model has not captured the underlying logic of the data
		it does not have  a strong predictive power with low accuracy
		Low train accuracy and low test accuracy
	Bias-Variance  
		The balance between underfitting and overfitting
	Early Stopping
		Cause a model to stop training to prevent overfitting
		Methods of usage
			- Preset number of epochs
				Very few epochs would cause an unsatisfactory result
				Can still lead to overfitting
			- Stop when updates become too small
				Stop when the relative decrese in the loss function is les than 0.001
				Saves computing power
				Can still lead to overfitting
			- Validation set
				Addresses overfitting
				It may take more time to fit
				Compares the validation and training loss and stops where both functions begins diverging


# Non-Linear Model
	Quadratic
	Exponential
	Logistic


# Logistic Regression
	This implies that the possible outcomes are not numerical but rather categorical
	This predicts the probability of an event occuring
	
	p(X) = [e(b0 + b1x1 +.....+bnxn)]/[1 + e(b0 + b1x1 +.....+bnxn)] .................Logistic Regression Model
		When transformed:: [p(X)]/[1-p(X)] = e(b0 + b1x1 +.....+bnxn) ................ Logit Regression Model
			[p(X)]/[1-p(X)] is called ODDS
			Therfore:: ODDS = e(b0 + b1x1 +.....+bnxn)
				Implies:: log(ODDS) = b0 + b1x1 +.....+bnxn
					Log of ODDS is equal to a linear model
		
	Where:: p(X) represents the 'probability of and event 'X' occuring
	

# Likelihood Function
	Estimates how likely a model describes the real underlying relationship of the variables
	The bigger the likelihood function, the higher the probability the model is correct
	* MLE (Maximum likelihood estimation)
		This tries to maximize the (log) likelihood function
	* LL_Null (Log likelihood-null)
		This is the log likelihood of a model with no independent variable
		y = b0
	* LLR (Log Likelihood ratio)
		Like F-statistic
		It measures if the model is statistically different from the LL_Null (ie useless model)
	* Pseudo R-Squared (Recall: AIC, BIC, McFadden's R-Squared[alternative name])
		A goog value is between 0.2 and 0.4
		

# Cluster Analysis
	Grouping data together based on simmilarities/diferences among/from them/others
	Cluster analysis is a multivariate statistical technique, that group's observations on the basis some of their features or variables they are described by.
	It is used to::
		* Explore Data
		* Identify Pattersn
	* Euclidean Distance
		This is the sum of the squared difference between the position/coordinates (x[i],y[i]) of 2 points
		* A centroid is the mid position between 2 points
	* K-Means
		'K' stands for the number of clusters we intend to identify
		The criterion for selecting optimal 'K' is the elbow method which measures the 'Within Cluster Sum of Squares' WCSS
			We want WCSS to be as low as possible to result to a suitable number of clusters for explanation
			Pros & Cons
					Limitations::													Remedies::
				We need to pick 'k'												Elbow method solves that
				Sensitive to initialization (seed initial position)				Use k_means++
				Sensitive to outliers											Remove outliers
				Produces spherical solutions
				standardization													Not to be used when initially known a variable is more important than others
	
	* Types of Clustering
		Flat (e.g. k-means)
			Has no hierarchy
		Hierarchical
			* Types::
				Agglomerative (Buttom-Up) [Much easier to solve mathematically]
					Begins with an individual observation in its own cluster and begins to pair up (reducing clusters) until a single cluster containing all observations is derived
					This is represented using a 'DENDROGRAM' chart/graph
						The bigger the distance between 2 observations/links, the bigger the difference in terms of their observed feature(s)
							Pros											Cons
						Shows all possible linkages						Scalability (Hard to read with excess data)
						Understand the data better						Computationally expensive (the more the observation, the slower it gets)
						No need to preset number of clusters
						Many method to perform this approach
				Divisive (Top-Down)
					Begins with all observation are in the same cluster; then broken down into 2,3,4.....,n clusters until each observation is a seperate cluster


# Classification
	Predicting an output category given an input data


# Scalar, Vector, Matrix, Tensor
	* Scalar (Tensor of Rank 0)
		Dimension is 1 X 1
		A single row and single column matrix
		A single number is a scalar e.g. [1]

	* Vector (Tensor of Rank 1)
		Dimension is (1 X n) OR (m X 1)
		A collection of scalars
		A single row OR single column matrix e.g. [1 2 4]
		Types::
			Row vector::
				Has a single row and multiple column e.g. [1 2 4]
			Column Vector::
			Has a single column and multiple rows e.g. [ 1
														 2 ]

	* Matrix (Tensor of Rank 2)
		Dimension is m X n
		This is a collection of vectors also a collection scalars
		A multiple row and multiple column matrix
		[ 1 2 3
		  3 5 7 ]
		  
	* Tensor (Rank 3)
		Dimension is k X m X n
		A collection of matrix
	
	Adding / subtracting 2 matrices requires both to have same dimension
	Dot product of vectors is the sum of the product of corresponding values
		A matrix of (m X n) can only be multiplied with a matrix (n X k) and this gives a (m X k) matrix result
		

# Machine Learning
	Types::
		* Supervised:
			The algorithm is provided with inputs and their corresponding desired outputs
			It learns how to produce outputs as close to the output
			Types:
				Classification:
					Provides Categorised outputs
					Loss function used is cross-entropy
						L(y,t) = -SUM(t[ln(y)]) :: Cross-entropy -> minus sum of t(targets * natural log of the outputs)
						Types:
							binary_crossentropy
								Case where there is binary encoding
							categorical_crossentropy
								Expects that the targets are already one-hot encoded
							sparse_categorical_crossentropy
								Applies one-hot encoding to the data
				Regression:
					Provides numerical outputs
					Loss function used is L2-norm
						L2-norm = SUM(y-t)^2 :: sum of the squared difference between the outputs and the targets
			The lower the loss fuction, the more accurate the model is
		* Unsupervised:
			Machine receives inputs, but there are no target outputs
			Therefore, the algorithm has no idea what the goal is.
			Instead, it find some sort of dependent's or underlying logic in the data provided
			It is especially useful when the goal is to split a dataset into a certain number of categories, which is unknow prior to implementation (clustering)
		* Reinforcement:
			A model is trained to act in an environment based on the rewards it receives.


# Deep Net
	* Parameters (e.g. weights, bias)
		Found by optimization
	* Hyperparameters (e.g. width, depth, learning rate, batch size, momentum rate, decay coefficient)
		Preset before optimization begins
	
	* Activator function / non-linearities are required by deep nets to find complex relationships through arbitraty functions
		It transform inputs into outputs of a different kind
		It helps in stacking layers
		The brain is the activation function that tells human to / not wear warm clothes when there is a linear temperature drop
		* Types/common kinds used (they are all monotonic,continuous and differentiable)
			Name                          Formula                                          Range
			sigmoid(logistic function)    sigma(a)=1/[1+exp^(-a)]                          (0,1)
			TanH(hyperbolic Tangent)      tanh(a)=[(exp^a)-(exp^-a)]/[(exp^a)+(exp^-a)]    (-1,1)
			ReLu (rectifier linear unit)  relu(a) max(0,a)                                 (0,infinity)
				This is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero
			softmax                       sigma{1}(a) = exp^a{i}/sumation{j}(exp^a{j})     (0,1)
			
			* sigmoid(logistic function)
				The input to the function is transformed into a value between 0.0 and 1.0
				Inputs that are much larger than 1.0 are transformed to the value 1.0, similarly, values much smaller than 0.0 are snapped to 0.0
				The shape of the function for all possible inputs is an S-shape from zero up through 0.5 to 1.0
			
			* TanH(hyperbolic Tangent)
				This is a similar shaped nonlinear activation function that outputs values between -1.0 and 1.0
				It was preferred over the sigmoid activation function as models that used it were easier to train and often had better predictive performance
				
					#* TanH and Sigmoid both saturate:
						This means that large values snap to 1.0 and small values snap to -1 or 0 for tanh and sigmoid respectively
						The functions are only really sensitive to changes around their mid-point of their input, such as 0.5 for sigmoid and 0.0 for tanh
							
			
			* ReLu (rectifier linear unit)
				This is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero
					It returns the value provided as input directly, or the value 0.0 if the input is 0.0 or less
				It looks and acts like a linear function, but is, in fact, a nonlinear function allowing complex relationships in the data to be learned
				It provides more sensitivity to the activation sum input and avoid easy saturation
					# code for ReLU
						def rectified(x):
							return max(0.0, x)
				
			
			* Softmax
				Transforms a bunch of arbitrarily large or small numbers into a valid probability diatribution
				Often used as the activation of the output layer in classification problems
				
	* Back propagation of errors
		* Forward propagation is the process of pushing inputs through the nets
		* Through back propagation, the algorithm identifies which weights leads to which errors
			- it adjusts the weights that have a bigger contribution to the errors by more than the weights with a smaller contribution
	
	# In buliding a neural net try
		- Adding/subtracting hidden layer
		- Increasing or decreasing  number hidden units per layer

			
# Learning Rate
	High enough::
		To reach the closest minimum in a rotational amount of time
	Low enough
		 To avoid oscillating around the minimum
	* Schedules
		Method 1
			Start from high initial learning rate (First 5 epochs [learning rate = 0.1])
			At some point, lower learning rate to avaod oscillation (Next 5 epochs [learning rate = 0.01])
			Around the end, select a small rate to get precise answer (Until end [learning rate = 0.001])
		Method 2 (Exponential Schedule)
			Smoothly reduces the learning rate
				100 epochs, 50<c<500
				1000 epochs, 500<c<5000
					c is decay coefficient
		Adaptive Learning Rates
			Types
				AdaGrad (Adaptive Gradient Algorithm)
					It dynamically varies the learning rate at each update and for every weight individually
				RMSProp (Root Mean Square Propagation)
		ADAM (Adaptive Moment Estimation)
			Updates RMSProp with a momentum rule
		


# Initialization
	Process in which the initial values of weights are set
	* Types
		Initialize randomly in uniform
		Choose a normal initializer from a '0' (zero) mean normal distribution and a standard deviation of '0.1'
		Xavia Initialization (Glorot Initialization)
			Uniform  Xavier Initialization
				Draw each weight (w) from a random uniform distribution in range(-x,x) for x = square_root(6/[ input+output])
			Normal Xavier Initialization
				Draw each weight (w) from a normal distribution with a mean of '0', and a standard deviation (sigma) = square_root(2/[ input+output])


# Stochastic Gradient Descent (SGD)
	Gradient Descent iterates over the whole dataset before updating the weights (very slow)
	SGD
		Related to batching concept and the weights are updated after every batch
			Spliting data into batches and updates the weights after every batch instead of each epoch
			Updates in real time inside a single epoch
			Most likely to stop at a local minimum rather thana  global one (depends on learning rate)
				Can be accounted for by factoring in momentum

				


# NB
	It is not always appropriate to standardize dummy variables because scaling has no effect on their predictive power; but when scaled they lose all the dummy meaning
	
	# Model Assumptions
		Linear							Logistic
		Linearity						Non-Linear
		No Endogeneity					No Endogeneity
		Normality & Homoscedastic		Normality & Homoscedastic
		No Autocorrelation				No Autocorrelation
		No Multicollinearity			No Multicollinearity
	
	# Setting up tensorflow for anaconda
		* Open Anaconda Prompt
		* type " conda create --name py3-TF2.0 python=3" # Create new environment to install tensorflow
		* type " y " and hit enter
		* type " conda activate py3TF-2.0 " # activate the environment
		* type " conda install tensorflow " # install tensorflow
		* type " y " and hit enter
		* type " conda install nb_conda_kernels "
		* type " y " and hit enter
		* type " conda deactivate "
		* type " conda install nb_conda_kernels "
		* type " y " and hit enter
		* type " pip install --upgrade tensorflow " upgrade tensorflow to latest version
		* type " pip install ipykernel # to ensure to see kernel in jupyter when started
		* Restart Jupyter
		
