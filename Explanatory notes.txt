# Function and methods
	Functions can take in multiple arguments/objects
		eg.
			function_name(arguments/object)
			
	Methods are applied at the end of a single object
		eg.
			object_name.method()


# Index and slicing
	Python position/index counts starts from '0' (zero)


# Data type identification
	Tuple = ()
	List = []
	Dictionary = {}


# Class, Object, Attributes
	Class: eg 								bike-maker, 		list class
	Object/instance: eg 							bikes, 			list
	attribute/properties: eg 						color, size, 		type(int,str,floats)
	Method is like a cosequential sequence applied to an object
		eg
										.turn_left()		.extend()
										.turn_right()		.index()


# SST / SSR / SSE
	# SST (sum of squares total) OR TSS (Total Sum of Squares)
		sumation of square diff between observed variavle and its mean
		Dispersion of observed variable around mean or measure of total variability of the data
		E(y(i) - ybar)^2

	# SSR (sum of squares Regressions) OR ESS (Explained Sum of Squares)
		sumation of square diff between predicted variavle and its mean
		Measure that describes how well a line fits the data
		E(yhat(i) - ybar)^2

	# SSE (sum of squares Error) OR RSS (Residual Sum of Squares)
		sumation of square diff between predicted variavle and its mean
		Measure the unexplained variability by the regression
		The smaller the error, the better the estimation power of regression
		E(y(i) - yhat(i))^2

	If SST = SSR....means regression captures all variability and is perfect
	
	SST = SSR + SSE
	
# R-Squared
	R^2 = SSR/SST
	This is a value/measure that quantifies the level of variability explained by the model
	It measures the goodness of fit of the model
	
# Adjusted R-Squared
	Always smaller than R-Squared
	This is a measure that penalizes a model for increasing the number of predictors used
	It is a basis for comparing models`````````````````````
	
# Linear Regression
	# Simple Linear Regression
		y = b0 + b1x
		
	# Multiple Linear Regression
		y = b0 + b1x1 + b2x2 +....... bnxn
		
	y: dependent variable/predicted
	x,x1,.....xn: independent variables/predictors
	b0: intercept
	b1,....bn: coefficient/slope

# Hypothesis
	Regression coefficients
		Null Hypothesis:- H0 = b = 0 :: b is any coefficient/slope of a model (not intercept(b0)
			 A p-value < 0.05 signifies to reject the null hypothesis, i.e b is significantly/statistically different from 0(zero)
			 
	Z - Statistic:- Follows a normal distribution
	T - Statistic:- Follows a student T distribution
	F - Statistic:- Follows F distribution
		The lower the F - Statistic, the less significant the model is
		This is used to test for overall significance of the model
			F-test:
				Null Hypothesis H0:b1=b2......bn=0
				Alternate Hypothesis H1: atleast one bi != 0
					It implies that if all bi = 0, then no independent variable matter
					A p-value < 0.05 signifies to reject the null hypothesis, i.e atliest one bi is significantly/statistically different from 0(zero)
					

	
