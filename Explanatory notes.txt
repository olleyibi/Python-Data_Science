# Function and methods
	Functions can take in multiple arguments/objects
		eg.
			function_name(arguments/object)
			
	Methods are applied at the end of a single object
		eg.
			object_name.method()


# Index and slicing
	Python position/index counts starts from '0' (zero)


# Data type identification
	Tuple = ()
	List = []
	Dictionary = {}


# Class, Object, Attributes
	Class: eg 															bike-maker, 		list class
	Object/instance: eg 												bikes, 				list
	attribute/properties: eg 											color, size, 		type(int,str,floats)
	Method is like a cosequential sequence applied to an object
		eg
																		.turn_left()		.extend()
																		.turn_right()		.index()


# Types of Analysis
	* Exploratory
		Getting acquainted with the data without explicitely trying to explain anything
		Searching for patterns
		Planning method to use
		* Techniques used include
			Data Visualization
			Descriptive Statistics
			Clustering
	* Confirmatory and Explanatory
		Both explain a phenomeno
		Confirm hypothesis
		Validate previous research
		* Techniques Used includes
			Hypothesis testing
			Regression Analysis


# Machine Learning
	Types::
		* Supervised:
			The algorithm is provided with inputs and their corresponding desired outputs
			It learns how to produce outputs as close to the output
			Types:
				Classification:
					Provides Categorised outputs
					Loss function used is cross-entropy
						L(y,t) = -SUM(t[ln(y)]) :: Cross-entropy -> minus sum of t(targets * natural log of the outputs)
				Regression:
					Provides numerical outputs
					Loss function used is L2-norm
						L2-norm = SUM(y-t)^2 :: sum of the squared difference between the outputs and the targets
			The lower the loss fuction, the more accurate the model is
		* Unsupervised:
			Machine receives inputs, but there are no target outputs
			Therefore, the algorithm has no idea what the goal is.
			Instead, it find some sort of dependent's or underlying logic in the data provided
			It is especially useful when the goal is to split a dataset into a certain number of categories, which is unknow prior to implementation (clustering)
		* Reinforcement:
			A model is trained to act in an environment based on the rewards it receives.


# SST / SSR / SSE
	# SST (sum of squares total) OR TSS (Total Sum of Squares)
		sumation of square diff between observed variavle and its mean
		Dispersion of observed variable around mean or measure of total variability of the data
		E(y(i) - ybar)^2

	# SSR (sum of squares Regressions) OR ESS (Explained Sum of Squares)
		sumation of square diff between predicted variavle and its mean
		Measure that describes how well a line fits the data
		E(yhat(i) - ybar)^2

	# SSE (sum of squares Error) OR RSS (Residual Sum of Squares)
		sumation of square diff between predicted variavle and its mean
		Measure the unexplained variability by the regression
		The smaller the error, the better the estimation power of regression
		E(y(i) - yhat(i))^2

	If SST = SSR....means regression captures all variability and is perfect
	SST = SSR + SSE


# Learning Rate
	High enough::
		To reach the closest minimum in a rotational amount of time
	Low enough
		 To avoid oscillating around the minimum

	
# R-Squared
	R^2 = SSR/SST
	This is a value/measure that quantifies the level of variability explained by the model
	It measures the goodness of fit of the model
	

# Adjusted R-Squared
	Always smaller than R-Squared
	This is a measure that penalizes a model for increasing the number of predictors used
	It is a basis for comparing models
		Adjusted R^2 = 1 - (1-R^2)*(n-1) / (n-p-1)
			n = number of observations
			p = number of predictors

	
# Linear Regression
	# Simple Linear Regression equations
		y = b0 + b1x :for every unit increase in x, y increases by b1
		y = b0 + b1[log(x)] :for every unit percent increase in x, y increases by b1
		log(y) = b0 + b1[log(x)] :for every unit percent percent increase in x, y increases by b1 percent
		log(y) = b0 + b1[log(x)] :for every unit increase in x, y increases by b1 percent
		
	# Multiple Linear Regression
		y = b0 + b1x1 + b2x2 +....... bnxn
		
	y: dependent variable/predicted
	x,x1,.....xn: independent variables/predictors
	b0: intercept
	b1,....bn: coefficient/slope
	
	A positive weight(b1,...bn) indicates that as a feature(x) increases in value, so does y
	A negative weight(b1,...bn) indicates that as a feature(x) decreases in value, so does y


# Hypothesis
	Regression coefficients
		Null Hypothesis:- H0 = b = 0 :: b is any coefficient/slope of a model (not intercept(b0)
			 A p-value < 0.05 signifies to reject the null hypothesis, i.e b is significantly/statistically different from 0(zero)
			 
	Z - Statistic:- Follows a normal distribution
	T - Statistic:- Follows a student T distribution
	F - Statistic:- Follows F distribution
		The lower the F - Statistic, the less significant the model is
		This is used to test for overall significance of the model
			F-test:
				Null Hypothesis H0:b1=b2......bn=0
				Alternate Hypothesis H1: atleast one bi != 0
					It implies that if all bi = 0, then no independent variable matter
					A p-value < 0.05 signifies to reject the null hypothesis, i.e atliest one bi is significantly/statistically different from 0(zero)


# Multicollinearity
	This is the occurrence of high intercorrelations among two or more independent variables in a multiple regression model
	This can be checked using VIF (variance inflation factor
		VIF = 1 :: no multicollinearity
		1 < VIF < 5 :: perfectly ok
		5,6,10 < VIF :: unacceptable (it varies from 5-10)
	

# Endogeneity
	Occurs when explanatory variables are omitted in the regression, which would result in the error term being correlated with the explanatory variables
					

# Homoscedastic
	This is a condition in which the variance of the residual, or error term, in a regression model is constant (no pattern when residuals are plotted)
	The most common fix for this is the log transformation


# Autocorrelation
	This refers to the degree of correlation between the values of the same variables across different observations in the data
	# Detecting it
		Manually:: This can be detected by plotting the residuals and looking for patterns in the plot, if none then no autocorrelation
		Using Durbin watson Test:: 		Usually between 0 and 4
										2 indicates no autocorelation
										<1 and >3  indicates the presence of autocorelation


# Standardization
	Also known as feature scaling or normalization
	Process of transforming data to a standard scale
	It involves subtracting the mean and dividing bt the standard deviation
	

# Overfitting and Underfitting
	Overfitting means a model has focused on the training set so muct that it 'missed the point'
		Random noise is captured
		High train accuracy and low test accuracy
		Solution: Split data into 2 (training and test set)
	Underfitting means the model has not captured the underlying logic of the data
		it does not have  a strong predictive power with low accuracy
		Low train accuracy and low test accuracy


# Non-Linear Model
	Quadratic
	Exponential
	Logistic


# Logistic Regression
	This implies that the possible outcomes are not numerical but rather categorical
	This predicts the probability of an event occuring
	
	p(X) = [e(b0 + b1x1 +.....+bnxn)]/[1 + e(b0 + b1x1 +.....+bnxn)] .................Logistic Regression Model
		When transformed:: [p(X)]/[1-p(X)] = e(b0 + b1x1 +.....+bnxn) ................ Logit Regression Model
			[p(X)]/[1-p(X)] is called ODDS
			Therfore:: ODDS = e(b0 + b1x1 +.....+bnxn)
				Implies:: log(ODDS) = b0 + b1x1 +.....+bnxn
					Log of ODDS is equal to a linear model
		
	Where:: p(X) represents the 'probability of and event 'X' occuring
	

# Likelihiid Function
	Estimates how likely a model describes the real underlying relationship of the variables
	The bigger the likelihood function, the higher the probability the model is correct
	* MLE (Maximum likelihood estimation)
		This tries to maximize the (log) likelihood function
	* LL_Null (Log likelihood-null)
		This is the log likelihood of a model with no independent variable
		y = b0
	* LLR (Log Likelihood ratio)
		Like F-statistic
		It measures if the model is statistically different from the LL_Null (ie useless model)
	* Pseudo R-Squared (Recall: AIC, BIC, McFadden's R-Squared[alternative name])
		A goog value is between 0.2 and 0.4
		

# Cluster Analysis
	Grouping data together based on simmilarities/diferences among/from them/others
	Cluster analysis is a multivariate statistical technique, that group's observations on the basis some of their features or variables they are described by.
	It is used to::
		* Explore Data
		* Identify Pattersn
	* Euclidean Distance
		This is the sum of the squared difference between the position/coordinates (x[i],y[i]) of 2 points
		* A centroid is the mid position between 2 points
	* K-Means
		'K' stands for the number of clusters we intend to identify
		The criterion for selecting optimal 'K' is the elbow method which measures the 'Within Cluster Sum of Squares' WCSS
			We want WCSS to be as low as possible to result to a suitable number of clusters for explanation
			Pros & Cons
					Limitations::													Remedies::
				We need to pick 'k'												Elbow method solves that
				Sensitive to initialization (seed initial position)				Use k_means++
				Sensitive to outliers											Remove outliers
				Produces spherical solutions
				standardization													Not to be used when initially known a variable is more important than others
	
	* Types of Clustering
		Flat (e.g. k-means)
			Has no hierarchy
		Hierarchical
			* Types::
				Agglomerative (Buttom-Up) [Much easier to solve mathematically]
					Begins with an individual observation in its own cluster and begins to pair up (reducing clusters) until a single cluster containing all observations is derived
					This is represented using a 'DENDROGRAM' chart/graph
						The bigger the distance between 2 observations/links, the bigger the difference in terms of their observed feature(s)
							Pros											Cons
						Shows all possible linkages						Scalability (Hard to read with excess data)
						Understand the data better						Computationally expensive (the more the observation, the slower it gets)
						No need to preset number of clusters
						Many method to perform this approach
				Divisive (Top-Down)
					Begins with all observation are in the same cluster; then broken down into 2,3,4.....,n clusters until each observation is a seperate cluster


# Classification
	Predicting an output category given an input data


# Scalar, Vector, Matrix, Tensor
	* Scalar (Tensor of Rank 0)
		Dimension is 1 X 1
		A single row and single column matrix
		A single number is a scalar e.g. [1]

	* Vector (Tensor of Rank 1)
		Dimension is (1 X n) OR (m X 1)
		A collection of scalars
		A single row OR single column matrix e.g. [1 2 4]
		Types::
			Row vector::
				Has a single row and multiple column e.g. [1 2 4]
			Column Vector::
			Has a single column and multiple rows e.g. [ 1
														 2 ]

	* Matrix (Tensor of Rank 2)
		Dimension is m X n
		This is a collection of vectors also a collection scalars
		A multiple row and multiple column matrix
		[ 1 2 3
		  3 5 7 ]
		  
	* Tensor (Rank 3)
		Dimension is k X m X n
		A collection of matrix
	
	Adding / subtracting 2 matrices requires both to have same dimension
	Dot product of vectors is the sum of the product of corresponding values
		A matrix of (m X n) can only be multiplied with a matrix (n X k) and this gives a (m X k) matrix result
		

# NB
	It is not always appropriate to standardize dummy variables because scaling has no effect on their predictive power; but when scaled they lose all the dummy meaning
	
	# Model Assumptions
		Linear						Logistic
		Linearity					Non-Linear
		No Endogeneity					No Endogeneity
		Normality & Homoscedastic			Normality & Homoscedastic
		No Autocorrelation				No Autocorrelation
		No Multicollinearity				No Multicollinearity
	
	# Setting up tensorflow for anaconda
		* Open Anaconda Prompt
		* type " conda create --name py3-TF2.0 python=3" # Create new environment to install tensorflow
		* type " y " and hit enter
		* type " conda activate py3TF-2.0 " # activate the environment
		* type " conda install tensorflow " # install tensorflow
		* type " y " and hit enter
		* type " conda install nb_conda_kernels "
		* type " y " and hit enter
		* type " conda deactivate "
		* type " conda install nb_conda_kernels "
		* type " y " and hit enter
		* type " pip install --upgrade tensorflow " upgrade tensorflow to latest version
		* type " pip install ipykernel # to ensure to see kernel in jupyter when started
		* Restart Jupyter
		
