{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "### 8. Adjust the learning rate. Try a value of 0.0001. Does it make a difference?\n",
    "\n",
    "** Solution **\n",
    "\n",
    "First, we have to define a custom optimizer (as we did in the TensorFlow intro).\n",
    "\n",
    "We create the custom optimizer with:\n",
    "\n",
    "    custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "Then we change the respective argument in model.compile to reflect this: \n",
    "\n",
    "    model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "Since the learning rate is lower than normal, we may need to adjust the max_epochs (to, say, 50). \n",
    "\n",
    "The result is basically the same, but we reach it much slower.\n",
    "\n",
    "While Adam adapts to the problem, if the orders of magnitude are too different, it may not have enough time to adjust accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" of deep learning because for most students it is the first deep learning algorithm they see.\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs). \n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "That's where we load and preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "\n",
    "def scale(image, label):\n",
    "    \n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    image /= 255.\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "\n",
    "# let's also shuffle the data\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the model\n",
    "When thinking about a deep learning algorithm, we mostly imagine building the model. So, let's do it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    \n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    \n",
    "    \n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "That's where we train the model we have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "540/540 - 7s - loss: 1.2689 - accuracy: 0.6816 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "540/540 - 6s - loss: 0.4529 - accuracy: 0.8850 - val_loss: 0.3684 - val_accuracy: 0.9018\n",
      "Epoch 3/50\n",
      "540/540 - 6s - loss: 0.3437 - accuracy: 0.9059 - val_loss: 0.3054 - val_accuracy: 0.9178\n",
      "Epoch 4/50\n",
      "540/540 - 7s - loss: 0.2977 - accuracy: 0.9164 - val_loss: 0.2745 - val_accuracy: 0.9280\n",
      "Epoch 5/50\n",
      "540/540 - 6s - loss: 0.2705 - accuracy: 0.9237 - val_loss: 0.2533 - val_accuracy: 0.9325\n",
      "Epoch 6/50\n",
      "540/540 - 7s - loss: 0.2513 - accuracy: 0.9285 - val_loss: 0.2353 - val_accuracy: 0.9367\n",
      "Epoch 7/50\n",
      "540/540 - 7s - loss: 0.2345 - accuracy: 0.9331 - val_loss: 0.2217 - val_accuracy: 0.9410\n",
      "Epoch 8/50\n",
      "540/540 - 7s - loss: 0.2191 - accuracy: 0.9379 - val_loss: 0.2092 - val_accuracy: 0.9435\n",
      "Epoch 9/50\n",
      "540/540 - 8s - loss: 0.2054 - accuracy: 0.9420 - val_loss: 0.1994 - val_accuracy: 0.9460\n",
      "Epoch 10/50\n",
      "540/540 - 7s - loss: 0.1939 - accuracy: 0.9444 - val_loss: 0.1891 - val_accuracy: 0.9498\n",
      "Epoch 11/50\n",
      "540/540 - 8s - loss: 0.1842 - accuracy: 0.9470 - val_loss: 0.1792 - val_accuracy: 0.9532\n",
      "Epoch 12/50\n",
      "540/540 - 8s - loss: 0.1757 - accuracy: 0.9495 - val_loss: 0.1709 - val_accuracy: 0.9548\n",
      "Epoch 13/50\n",
      "540/540 - 8s - loss: 0.1671 - accuracy: 0.9517 - val_loss: 0.1646 - val_accuracy: 0.9557\n",
      "Epoch 14/50\n",
      "540/540 - 8s - loss: 0.1611 - accuracy: 0.9541 - val_loss: 0.1604 - val_accuracy: 0.9593\n",
      "Epoch 15/50\n",
      "540/540 - 7s - loss: 0.1541 - accuracy: 0.9559 - val_loss: 0.1523 - val_accuracy: 0.9602\n",
      "Epoch 16/50\n",
      "540/540 - 6s - loss: 0.1474 - accuracy: 0.9576 - val_loss: 0.1489 - val_accuracy: 0.9595\n",
      "Epoch 17/50\n",
      "540/540 - 6s - loss: 0.1448 - accuracy: 0.9584 - val_loss: 0.1434 - val_accuracy: 0.9625\n",
      "Epoch 18/50\n",
      "540/540 - 7s - loss: 0.1386 - accuracy: 0.9605 - val_loss: 0.1395 - val_accuracy: 0.9637\n",
      "Epoch 19/50\n",
      "540/540 - 6s - loss: 0.1332 - accuracy: 0.9621 - val_loss: 0.1359 - val_accuracy: 0.9640\n",
      "Epoch 20/50\n",
      "540/540 - 6s - loss: 0.1290 - accuracy: 0.9631 - val_loss: 0.1315 - val_accuracy: 0.9648\n",
      "Epoch 21/50\n",
      "540/540 - 6s - loss: 0.1249 - accuracy: 0.9649 - val_loss: 0.1309 - val_accuracy: 0.9650\n",
      "Epoch 22/50\n",
      "540/540 - 5s - loss: 0.1219 - accuracy: 0.9652 - val_loss: 0.1244 - val_accuracy: 0.9660\n",
      "Epoch 23/50\n",
      "540/540 - 6s - loss: 0.1185 - accuracy: 0.9660 - val_loss: 0.1236 - val_accuracy: 0.9665\n",
      "Epoch 24/50\n",
      "540/540 - 5s - loss: 0.1143 - accuracy: 0.9674 - val_loss: 0.1184 - val_accuracy: 0.9685\n",
      "Epoch 25/50\n",
      "540/540 - 6s - loss: 0.1119 - accuracy: 0.9675 - val_loss: 0.1159 - val_accuracy: 0.9680\n",
      "Epoch 26/50\n",
      "540/540 - 5s - loss: 0.1091 - accuracy: 0.9689 - val_loss: 0.1149 - val_accuracy: 0.9690\n",
      "Epoch 27/50\n",
      "540/540 - 6s - loss: 0.1056 - accuracy: 0.9700 - val_loss: 0.1113 - val_accuracy: 0.9695\n",
      "Epoch 28/50\n",
      "540/540 - 6s - loss: 0.1029 - accuracy: 0.9706 - val_loss: 0.1088 - val_accuracy: 0.9697\n",
      "Epoch 29/50\n",
      "540/540 - 6s - loss: 0.1007 - accuracy: 0.9709 - val_loss: 0.1067 - val_accuracy: 0.9702\n",
      "Epoch 30/50\n",
      "540/540 - 7s - loss: 0.0976 - accuracy: 0.9723 - val_loss: 0.1049 - val_accuracy: 0.9713\n",
      "Epoch 31/50\n",
      "540/540 - 6s - loss: 0.0949 - accuracy: 0.9727 - val_loss: 0.1037 - val_accuracy: 0.9715\n",
      "Epoch 32/50\n",
      "540/540 - 5s - loss: 0.0937 - accuracy: 0.9728 - val_loss: 0.1015 - val_accuracy: 0.9723\n",
      "Epoch 33/50\n",
      "540/540 - 6s - loss: 0.0909 - accuracy: 0.9739 - val_loss: 0.0977 - val_accuracy: 0.9733\n",
      "Epoch 34/50\n",
      "540/540 - 6s - loss: 0.0889 - accuracy: 0.9744 - val_loss: 0.0950 - val_accuracy: 0.9743\n",
      "Epoch 35/50\n",
      "540/540 - 6s - loss: 0.0882 - accuracy: 0.9744 - val_loss: 0.0968 - val_accuracy: 0.9735\n",
      "Epoch 36/50\n",
      "540/540 - 6s - loss: 0.0841 - accuracy: 0.9762 - val_loss: 0.0934 - val_accuracy: 0.9740\n",
      "Epoch 37/50\n",
      "540/540 - 5s - loss: 0.0833 - accuracy: 0.9759 - val_loss: 0.0910 - val_accuracy: 0.9745\n",
      "Epoch 38/50\n",
      "540/540 - 5s - loss: 0.0811 - accuracy: 0.9769 - val_loss: 0.0905 - val_accuracy: 0.9757\n",
      "Epoch 39/50\n",
      "540/540 - 5s - loss: 0.0793 - accuracy: 0.9772 - val_loss: 0.0883 - val_accuracy: 0.9763\n",
      "Epoch 40/50\n",
      "540/540 - 5s - loss: 0.0784 - accuracy: 0.9771 - val_loss: 0.0869 - val_accuracy: 0.9765\n",
      "Epoch 41/50\n",
      "540/540 - 5s - loss: 0.0770 - accuracy: 0.9776 - val_loss: 0.0863 - val_accuracy: 0.9763\n",
      "Epoch 42/50\n",
      "540/540 - 6s - loss: 0.0753 - accuracy: 0.9787 - val_loss: 0.0853 - val_accuracy: 0.9765\n",
      "Epoch 43/50\n",
      "540/540 - 6s - loss: 0.0734 - accuracy: 0.9786 - val_loss: 0.0836 - val_accuracy: 0.9768\n",
      "Epoch 44/50\n",
      "540/540 - 5s - loss: 0.0716 - accuracy: 0.9797 - val_loss: 0.0822 - val_accuracy: 0.9762\n",
      "Epoch 45/50\n",
      "540/540 - 6s - loss: 0.0705 - accuracy: 0.9798 - val_loss: 0.0806 - val_accuracy: 0.9783\n",
      "Epoch 46/50\n",
      "540/540 - 6s - loss: 0.0693 - accuracy: 0.9802 - val_loss: 0.0790 - val_accuracy: 0.9788\n",
      "Epoch 47/50\n",
      "540/540 - 7s - loss: 0.0678 - accuracy: 0.9805 - val_loss: 0.0775 - val_accuracy: 0.9782\n",
      "Epoch 48/50\n",
      "540/540 - 7s - loss: 0.0668 - accuracy: 0.9807 - val_loss: 0.0763 - val_accuracy: 0.9785\n",
      "Epoch 49/50\n",
      "540/540 - 9s - loss: 0.0645 - accuracy: 0.9815 - val_loss: 0.0776 - val_accuracy: 0.9775\n",
      "Epoch 50/50\n",
      "540/540 - 9s - loss: 0.0634 - accuracy: 0.9819 - val_loss: 0.0758 - val_accuracy: 0.9785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x237cd972b70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "As we discussed in the lectures, after training on the training data and validating on the validation data, we test the final prediction power of our model by running it on the test dataset that the algorithm has NEVER seen before.\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset. \n",
    "\n",
    "The test is the absolute final instance. You should not test before you are completely done with adjusting your model.\n",
    "\n",
    "If you adjust your model after testing, you will start overfitting the test dataset, which will defeat its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 4s 4s/step - loss: 0.1049 - accuracy: 0.96 - 4s 4s/step - loss: 0.1049 - accuracy: 0.9675"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.10. Test accuracy: 96.75%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
